{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b74997b-d320-4096-9225-2ac330923cd3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Step 1 - Acquiring Data\n",
    "\n",
    "In this module, we will start our process to develop a machine learning model. The very first step is to obtain our data, and there are multiple ways to do that, depending on the type of data you're working with. By the end of this module, you will be able to:\n",
    "\n",
    "* Identify multiple sources of data\n",
    "* Identify different formats in which data can be found\n",
    "* Use multiple Python libraries to collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec200e86-e68d-44a5-bdaf-a6f99a5c08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load our libraries\n",
    "import sqlite3\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364208c-31ff-4930-b465-9230974bf9a8",
   "metadata": {},
   "source": [
    "## Using traditional data formats\n",
    "\n",
    "A lot of data can be found in open websites ready to be downloaded. Some of the most common formats in which data can be found are:\n",
    "\n",
    "* Text files (.txt)\n",
    "* Comma-Separated Values (.csv)\n",
    "* Excel (.xls)\n",
    "\n",
    "Let's load an example of each of them using Python. For this first part, we have previously downloaded 3 datasets publicly available at [data.gov](https://data.gov/).\n",
    "\n",
    "### Text files\n",
    "\n",
    "They can easily be opened in Python through the open function. The following file, represents the probability of female death by age and year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a968a302-2bd3-4e5b-b5ec-bdf39df1ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/female_death_probabilities.txt\", \"r\")\n",
    "f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c5383-7e74-4ccb-8528-c44247952a3d",
   "metadata": {},
   "source": [
    "We have read our file. However, it is not yet a dataset, but a string with a lot of lines. In most text files, some extra transformations must be made in order to have obtain functional dataframes. The following code achieves that by:\n",
    "1. Creating a dictionary that will eventually become our dataset.\n",
    "2. Extracting the lines and columns from the text.\n",
    "3. Iterating over the lines (starting from line 3), to extract the values and append them to the df.\n",
    "4. Creating a pandas dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d1f342-aecd-4c15-8121-ec753e93da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/female_death_probabilities.txt\", \"r\")\n",
    "data = {\n",
    "    \"Year\": [],\n",
    "    \"Age\": [],\n",
    "    \"Probability_of_Death\": []\n",
    "}\n",
    "text_data = f.read()\n",
    "lines = text_data.strip().split('\\n')\n",
    "columns = lines[1].split()\n",
    "\n",
    "for line in lines[2:]:\n",
    "    values = line.split()\n",
    "    year = values[0]\n",
    "    probabilities = values[2:]\n",
    "    \n",
    "    for age, prob in zip(columns[2:], probabilities):\n",
    "        data[\"Year\"].append(year)\n",
    "        data[\"Age\"].append(int(age))\n",
    "        data[\"Probability_of_Death\"].append(float(prob))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f338e68-8747-4ab9-8aa4-12dbac4d5be4",
   "metadata": {},
   "source": [
    "As we can see, now we have a dataframe we can work with.\n",
    "\n",
    "### CSV Files\n",
    "\n",
    "A very popular type of file with which you might already be familiar is the Comma-Separated Values format. It is very likely that you have loaded a csv file in an Excel spreadsheet at this point. To read this type of files and load them as a dataframe, we can simply use the *read_csv* function from Pandas. The following dataset shows the population of electric vehicles in the state of Washington."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c1d01-2f36-4eb1-be63-63b3654a25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a csv file example https://catalog.data.gov/dataset/electric-vehicle-population-data\n",
    "vehicle_df = pd.read_csv(\"data/Electric_Vehicle_Population_Data.csv\")\n",
    "vehicle_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd2426-86c1-4747-a91b-32bdd4c030d9",
   "metadata": {},
   "source": [
    "## Accessing traditional databases\n",
    "\n",
    "Sometimes, data exists in a database format and follows the [relational database model](https://en.wikipedia.org/wiki/Relational_model). This model is the state of the art in most of today's database management systems.\n",
    "\n",
    "The default language to connect with a relational database and retrieve data from it is the **Structured Query Language (SQL)**. In Python, you can explore databases and execute SQL queries through the [sqlite3](https://docs.python.org/3/library/sqlite3.html) library. In the following cells, we will execute some queries using the open Chinook database. You can learn more about this dataset on this [link](https://github.com/lerocha/chinook-database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbc33b-fed6-4ca6-89b2-c606be51626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we establish a connection with the database\n",
    "conn = sqlite3.connect(\"data/Chinook_Sqlite.sqlite\")\n",
    "\n",
    "# The next step is to create cursor object to interact with the database\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# The very first query that we will execute, will show the different tables that are part of our database\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "\n",
    "# Now, we fetch the results\n",
    "tables = cursor.fetchall()\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1798ffc1-dcb9-45f7-b5dc-d254451d7fad",
   "metadata": {},
   "source": [
    "As we can see, we have multiple tables in our database. Let's now perform some example queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7241cac-b7e1-49f5-b77a-34dafcb63b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all the columns information from the Album table\n",
    "cursor.execute(\"PRAGMA table_info(Customer);\")\n",
    "columns = cursor.fetchall()\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8038b2b7-5034-4b66-9cee-7aed0b7be55b",
   "metadata": {},
   "source": [
    "Let's briefly summarize what each element of each tuple means:\n",
    "\n",
    "1. Index 0 is the index of the column\n",
    "2. Index 1 is the of the column\n",
    "3. Index 2 is the data type of the column\n",
    "4. Index 3 is a dummy which indicates if the column allows NULL values (1 for TRUE, 0 for FALSE)\n",
    "5. Index 4 is the default value of the column\n",
    "6. Index 5 is a dummy which indicates if the column is a primary key (1 for TRUE, 0 for FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3183cd-61b6-4b9b-9198-65f820e6a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 3 rows of the Customer table\n",
    "cursor.execute(\"SELECT * FROM Customer LIMIT 3;\")\n",
    "rows = cursor.fetchall()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc5f483-8258-41f1-b436-26e190ec8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the Name and email of the Customers where Country = Germany\n",
    "cursor.execute(\"SELECT FirstName, Email FROM Customer WHERE Country = 'Germany'\")\n",
    "names = cursor.fetchall()\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dbc91a-56de-4471-a97e-375b92b14309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 3 invoices where the total was equal or higher than 10.\n",
    "cursor.execute(\"SELECT * FROM Invoice WHERE Total >= 10 ORDER BY Total DESC LIMIT 3\")\n",
    "invoices = cursor.fetchall()\n",
    "invoices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b61d595-4fef-4e20-93a5-b3f9de7242d4",
   "metadata": {},
   "source": [
    "We have performed a few SQL queries to give you a sense on how to retrieve data from a traditional database. However, this is not a SQL focused module. If you want to learn more SQL, you can follow some of the following links:\n",
    "\n",
    "* [Geeks for Geeks](https://www.geeksforgeeks.org/sql-tutorial/)\n",
    "* [Khan Academy](https://www.khanacademy.org/computing/computer-programming/sql)\n",
    "* [W3 Schools](https://www.w3schools.com/sql/)\n",
    "\n",
    "## Accessing NoSQL databases\n",
    "\n",
    "Many open databases follow a non-relational format or NoSQL. Most of these databases follow a [JSON](https://en.wikipedia.org/wiki/JSON) format and can be accessed through the use of an [API](https://en.wikipedia.org/wiki/API). \n",
    "\n",
    "Let's look an example on how to retrieve a database through an API. For the following example, we are going to use the OpenWeather platform, which offers updated weather data for multiple locations around the world.\n",
    "\n",
    "In most of the cases, in order to retrieve data, you need first to obtain an API key, so the server can recognize you as a validate user and allow you to collect the data. OpenWeather is not the exception. Before starting the coding section, you'll need to create an account to obtain an API key:\n",
    "* [OpenWeather Sign Up](https://home.openweathermap.org/users/sign_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70393010-db77-4acd-b237-8d754e3b1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting Weather Data from OpenWeather\n",
    "api_key = \"4722cd5944f586c5451f6e105a65e129\" # Replace with you API Key\n",
    "\n",
    "# For this example, we are only going to retrieve today's weather data for the city of San Diego\n",
    "city = 'San Diego' # You can test with a different city\n",
    "country = 'US'\n",
    "\n",
    "url = f'http://api.openweathermap.org/data/2.5/weather?q={city},{country}&appid={api_key}'\n",
    "\n",
    "# Send a GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200: # We verify that our request was successful\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(response.status_code) # In case our request was not successful, we return the status code\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21d02b-fcf1-4009-ad64-81780d92b42b",
   "metadata": {},
   "source": [
    "As you can see, we collected our data in JSON format. However, this data is hard to work with in the current format. Let's do some transformations to use the data we need in a pandas dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7368f79-e3fb-4a25-b937-4f57e84f02aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to keep the name of the city, country, temperature, humidity and the description of the weather \n",
    "weather_data = {\n",
    "        'City': data['name'],\n",
    "        'Country': data['sys']['country'],\n",
    "        'Temperature (Celsius)': data['main']['temp'] - 273.15,  # Data comes in Kelvin by default. We convert to Celsius.\n",
    "        'Humidity (%)': data['main']['humidity'],\n",
    "        'Description': data['weather'][0]['description']\n",
    "    }\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame([weather_data])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f165856-ea75-4969-9c95-356a0c339275",
   "metadata": {},
   "source": [
    "In this example, we only collected data for a single city. However, you can think of a case in which you could retrieve data from a lot of cities and multiple dates to build a huge dataset.\n",
    "\n",
    "## Web-Scrapping\n",
    "\n",
    "Another way to collect data is through Web-Scrapping, which refers to the technique of directly collecting data from websites. A common python library to perform this task is [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) \n",
    "\n",
    "Imagine for example, that you wanted to analyze the sentiment and narrative of some of the most important news websites through the titles of their articles. In the following code, we are going to collect all the article titles from the BBC News main page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b2c30-ce33-449a-b6bf-574f115039df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web-Scrapping job to collect the titles of the news.\n",
    "url = 'https://www.bbc.com/news'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    article_titles = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "    for title in article_titles:\n",
    "        print(title.text.strip())\n",
    "else:\n",
    "    print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba03a2d6-4de2-43f4-b492-98728d05ba94",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's briefly recap what we did in the previous code: \n",
    "1. We defined the url of the page to parse and requested that page, receiving a response object.\n",
    "2. We created our soup object as a HTML parser.\n",
    "3. We requested the parser to find all the header objects. We placed all those objects in a list.\n",
    "4. We printed the text of each of those objects.\n",
    "   \n",
    "You can think of more extended versions of the previous exercise, like extracting the full articles and extracting data from multiple news sites."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
