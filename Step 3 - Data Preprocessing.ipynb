{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85a76a5f-514f-4911-839f-078758648dbe",
   "metadata": {},
   "source": [
    "# Step 3 - Data Preprocessing\n",
    "\n",
    "In the previous 2 modules:\n",
    "* Learned to acquire data through different methods\n",
    "* Discovered multiple formats in which data can be found and how to interact with each of them\n",
    "* Performed exploratory data analysis \n",
    "\n",
    "Now we will give you a brief introduction on data preprocessing. By the end of this module you will be able to\n",
    "\n",
    "1. Handle missing values and outliers\n",
    "2. Perform data transformations\n",
    "3. Apply feature selection techniques\n",
    "\n",
    "For this module we are going to use a different dataset from the previous 2 modules. We are going to use the Credit Approval dataset from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/). This data contains information regarding credit card applications. It is important to notice that the names of the features are not displayed in order to protect the confidentiality of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad3c4fc-91f8-4947-9e97-0c090048f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by loading the packages we are going to work with\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf48085-d1c2-4e77-9b64-5fe514d32d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our data\n",
    "credit_approval = fetch_ucirepo(id=27) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = credit_approval.data.features \n",
    "y = credit_approval.data.targets \n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc9927-0332-45bf-8f38-87dc3edd838f",
   "metadata": {},
   "source": [
    "## Missing values and outliers\n",
    "\n",
    "One of the most commonly encountered problems is the presence of missing values. Some of the ways we can threat these observations are the following:\n",
    "\n",
    "* Remove observations: Simply remove the observations for which relevant data is missed. \n",
    "* Median imputations (continous features): Replace missing values with the median of the distribution, particullarly if the distribution of the data is skewed.\n",
    "* Mean imputation (continous features): Replace missing values with the mean of the distribution, particullarly if the data is normally distributed.\n",
    "\n",
    "Outliers represent cases in which a data point possess a value that significantly differs from the rest of the observations. One way to deal with these outliers, is to eliminate those data points in which the value is more than a certain number of standard deviations from the mean, or in other words, if the z-score of the observation is above a certain threshold.\n",
    "\n",
    "Now, let's see some code to handle these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d32d7-3740-412c-a1e3-e9161fe6aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we identify the presence of missing values\n",
    "for x in X.columns:\n",
    "    print(f\"The column {x} has missing values: {X[x].isna().any()}\")\n",
    "print(f\"\\n{len(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f7504-e416-4681-a1b2-036122763161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any row that has a missing value\n",
    "X_clean = X.dropna() # This single line of code removes any row with a missing value in at least one column\n",
    "for x in X_clean.columns:\n",
    "    print(f\"The column {x} has missing values: {X_clean[x].isna().any()}\")\n",
    "print(f\"\\n{len(X_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5309e8-ff9c-4826-9334-89e6f93f4fcc",
   "metadata": {},
   "source": [
    "As we see, the second version of our features dataframe has no missing values in any of the columns. However, we also lost 5% of the observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b9b79-34e5-423a-a60e-b1a4b585453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing mean/median imputations\n",
    "X_imp = X.copy() # For this example, we are going to work with a copy\n",
    "floats = X_imp.select_dtypes(include=['float']) # We identify the columns with floating point values\n",
    "print(f\"Continous features: {floats.columns}\\n\")\n",
    "for x in floats:\n",
    "    if X_imp[x].isna().any():  # Check if the column has missing values\n",
    "        mean = X_imp[x].mean() # Replace .mean() to impute the median\n",
    "        X_imp[x].fillna(mean, inplace=True)\n",
    "        \n",
    "for x in X_imp.columns:\n",
    "    print(f\"The column {x} has missing values: {X_imp[x].isna().any()}\")\n",
    "print(f\"\\n{len(X_imp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cca019-f351-4d5c-aa1c-42a887c83a01",
   "metadata": {},
   "source": [
    "As we can see, none of the columns with continous features has missing values anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84689696-3178-4673-ba5d-5794b5b33891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling outliers \n",
    "\n",
    "threshold = 3\n",
    "valid_rows = [] # This list will store the rows with missing values\n",
    "\n",
    "for x in floats:\n",
    "    z_score = stats.zscore(X_clean[x])\n",
    "    valid = abs(z_score) <= threshold\n",
    "    valid_rows.append(valid)\n",
    "\n",
    "# Combine the outlier masks for all columns\n",
    "combined = pd.concat(valid_rows, axis=1).all(axis=1)\n",
    "combined\n",
    "\n",
    "# Remove rows with outliers\n",
    "X_clean = X_clean[combined]\n",
    "X_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae85d2-5f51-4a1d-bd41-bc99539650a5",
   "metadata": {},
   "source": [
    "## Data Transformations\n",
    "\n",
    "Sometimes we need to transform our data in order to make it useful. Let's explore some common techniques to transform our data. \n",
    "\n",
    "## Dummies\n",
    "\n",
    "For most of models, it is important to transform our categorical variables into a series binary columns (or dummies) to obtain a numerical representation of the presence or absence of each category. Let's see how to do that in the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e688f-2649-4e07-a467-3e1e27bb2340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummies\n",
    "categorical = X.select_dtypes(include=['object']).columns# First, we identify the non-numerical features\n",
    "X_dummies = pd.get_dummies(X[categorical], prefix=categorical) # Create dummies\n",
    "X_dummies = pd.concat([X, X_dummies], axis=1) # Concatenate dataframes\n",
    "X_dummies.drop(categorical, axis=1, inplace=True) # Keep only the dummies, as well as the original continous features\n",
    "\n",
    "X_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462992d-e3df-4f65-9218-433d0bc68ed0",
   "metadata": {},
   "source": [
    "In this case, we have a boolean representation for each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f84a1-456c-4c49-b8ef-481fb4d9ca6d",
   "metadata": {},
   "source": [
    "## Interactions\n",
    "\n",
    "For some models, like linear regression, the use of interactions between features might provide more information to the model and improve performance. \n",
    "\n",
    "In pandas, creating interaction is not a complicated task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f92a88-4c02-4283-bcfb-a8148f04eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction\n",
    "\n",
    "X_int = X.copy() \n",
    "\n",
    "X_int['A2*3'] = X_int['A2']*X_int['A3'] # We create an interaction feature of A2 and A3 by simply multiplying the columns.\n",
    "X_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c6a71b-83c2-4bb6-bcf6-527d921725a2",
   "metadata": {},
   "source": [
    "## Data normalization and standardization\n",
    "\n",
    "Another type of data transformation are the normalization and standardization of the continous features. These techniques help us reduce the impact of the outliers, as well as to improve the performance of our models.\n",
    "\n",
    "* Normalization: In this method, we set all values between 0 and 1, with the minimum value assigned 0 and the maximum 1, with the rest of the values being transformed between 0 and 1.\n",
    "* Standardization: In this method, all values are centered around the mean, with the mean receiving a value of 0. Different from the previous one, there is no default range in this method.\n",
    "\n",
    "The *sklearn* library offers a convenient way to perform these tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78264c2-6d62-4af2-86db-549c791d5a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize continous features\n",
    "X_norm = X.copy()  # We are going to work with a copy\n",
    "X_floats = X[floats.columns]\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_norm[floats.columns] = scaler_minmax.fit_transform(X_floats)\n",
    "X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83769746-8ed2-442d-ac9c-e279a64f1323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize continous features\n",
    "X_std = X.copy()  # We are going to work with a copy\n",
    "X_floats = X[floats.columns]\n",
    "scaler_standard = StandardScaler()\n",
    "X_std[floats.columns] = scaler_standard.fit_transform(X_floats)\n",
    "X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ef32f-8064-4802-bf86-092670a15218",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "When we work with big datasets containing lots of features, it is important to only work with features that impact the prediction of our target. By doing this, we prevent overfitting our model, and we also use our computational resources more efficiently. The [sklearn feature selection](https://scikit-learn.org/stable/modules/feature_selection.html) module offers a series of feature selection methods which are convinient for this task. We will briefly explore some of them:\n",
    "\n",
    "* Variance Threshold: This method estimates the probability for each feature of having a variance different from 0. We set a threshold for this probability and remove all features below that threshold.\n",
    "* KBest: This method calculates a score (based on a function) between our features and our target, and select the *k* number of features that scored the highest. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5d44d-56d2-4204-9bf2-65f2972613df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Threshold\n",
    "X_floats = X_std.copy()\n",
    "X_floats = X_floats[floats.columns]\n",
    "for x in floats: # Our X_std still had some missing values. We are going to imput the mean. \n",
    "    if X_floats[x].isna().any():  # Check if the column has missing values\n",
    "        mean = X_floats[x].mean() # Replace .mean() to impute the median\n",
    "        X_floats[x].fillna(mean, inplace=True)\n",
    "selector = VarianceThreshold(threshold=0.982) # We can adjust the threshold. We are using an arbitrary high threshold for demonstrative purposes.\n",
    "X_var = selector.fit_transform(X_floats)\n",
    "selected_columns = X_floats.columns[selector.get_support()]\n",
    "X_var = pd.DataFrame(X_var, columns=selected_columns)\n",
    "print(X_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1168e7-e69e-439b-8371-668f572b7d0a",
   "metadata": {},
   "source": [
    "As we see, our features dataframe now has less features. In our threshold, we asked to remove all features with a probability (p-value) of having a variance equal to 0 of 1.82% or higher (which is a very strick threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dae05f-ced9-4afb-a6fb-8abe126a7110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Best\n",
    "y_bin = np.where(y[\"A16\"]==\"+\",1,0) # The current dtype of y is object. We need to convert it into a binary series.\n",
    "k_selector = SelectKBest(f_classif, k=2) # K = 2 because we only want to keep 2 features.\n",
    "X_k = k_selector.fit_transform(X_floats, y_bin)\n",
    "k_columns = X_floats.columns[k_selector.get_support()]\n",
    "X_k = pd.DataFrame(X_k, columns=k_columns)\n",
    "X_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02554fa6-df40-4325-ac99-08c0f177a0be",
   "metadata": {},
   "source": [
    "The two features with the highest scores using the F-statistic as our scoring method, are A8 and A3. Therefore, the method returns a dataframe with those 2 columns.\n",
    "\n",
    "We invite you to take a look at more methods trough the official documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c82d6-3551-4ba4-82ed-bedef4fe570d",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "Another helpful way to improve the performance of our models and take better advantage of our computational resources is by performing dimensionality reduction to our data. This technique implies the transformation of our data into a lower dimension space, while performing the most relevant information. A very common approach for this task is the [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis). \n",
    "\n",
    "The next cell shows the implementation of a code using *sklearn*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0421f-c998-45b3-836b-00fda9590c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "X_pca = X_floats.copy()\n",
    "\n",
    "pca = PCA(n_components=2) # We are going to reduce our data into a 2D space. \n",
    "X_pca = pca.fit_transform(X_pca)\n",
    "X_pca = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(2)])\n",
    "print(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9092d5-deac-49f2-acf3-a48ad8741ca8",
   "metadata": {},
   "source": [
    "As we see, our data was transformed and reduced into two components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
